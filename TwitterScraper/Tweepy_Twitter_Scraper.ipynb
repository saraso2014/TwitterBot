{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xh92xbMkLy28"
   },
   "source": [
    "# Scraper for Twitter using Tweepy\n",
    "\n",
    "Package Github: https://github.com/tweepy/tweepy\n",
    "\n",
    "Package Documentation: https://tweepy.readthedocs.io/en/latest/\n",
    "\n",
    "### Notebook Author: Martin Beck"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 213
    },
    "colab_type": "code",
    "id": "90OU2SDJL2Q9",
    "outputId": "89d239d4-dc97-43c7-fff0-cbbe793bf094"
   },
   "outputs": [],
   "source": [
    "# Pip install Tweepy if you don't already have the package\n",
    "# !pip install tweepy\n",
    "\n",
    "# Imports\n",
    "import tweepy\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "%load keys.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5q3dtxauP0KR"
   },
   "source": [
    "## Credentials and Authorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4NcOQy9XM5hR"
   },
   "outputs": [],
   "source": [
    "# Credentials\n",
    "consumer_key = \"m8rPuAqDoxlp21cqf42OaCVNh\"\n",
    "consumer_secret = \"xnXprDHDJXgflYHENr1IrESBxWlZO2nFbaCCKiNuMmZMGltZcK\"\n",
    "access_token = \"208399633-4hjnFJvf2NOh0naUbwY3pm6TPpFX0guyH7klZKOH\"\n",
    "access_token_secret = \"rQc560zkbo8CdmzUa6g75xD4XwTnsSOHTiNRzOgLMsVjs\"\n",
    "\n",
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "api = tweepy.API(auth,wait_on_rate_limit=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LvBbNQXgM3QI"
   },
   "source": [
    "## Query by Username\n",
    "Creation of queries using Tweepy API\n",
    "\n",
    "Function is focused on completing the query then providing a CSV file of that query using pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fguMqU2ifc5h"
   },
   "outputs": [],
   "source": [
    "tweets = []\n",
    "\n",
    "def username_tweets_to_csv(username,count):\n",
    "    try: \n",
    "    # Pulling individual tweets from query\n",
    "        for tweet in api.user_timeline(id=username, count=count):\n",
    "\n",
    "            # Adding to list that contains all tweets\n",
    "            tweets.append((tweet.created_at,tweet.id,tweet.text, tweet.user.location))\n",
    "\n",
    "            # Creation of dataframe from tweets list\n",
    "            tweetsdf = pd.DataFrame(tweets,columns=['Datetime', 'Tweet Id', 'Text', 'Location'])\n",
    "\n",
    "            # Converting dataframe to CSV\n",
    "            tweetsdf.to_csv('{}-tweets.csv'.format(username)) \n",
    "\n",
    "    except BaseException as e:\n",
    "          print('failed on_status,',str(e))\n",
    "          time.sleep(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jFe9EonmM6u9"
   },
   "source": [
    "## Query by Text Search\n",
    "Function is focused on completing the query then providing a CSV file of that query using pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1hOeCFq6M83k"
   },
   "outputs": [],
   "source": [
    "tweets = []\n",
    "\n",
    "def text_query_to_csv(text_query, count):\n",
    "    try:\n",
    "    # Pulling individual tweets from query\n",
    "        for tweet in api.search(q=text_query, count=count):\n",
    "\n",
    "          # Adding to list that contains all tweets\n",
    "          tweets.append((tweet.created_at,tweet.id,tweet.text,tweet.coordinates,tweet.user.location, tweet.user.geo_enabled))\n",
    "\n",
    "          # Creation of dataframe from tweets list\n",
    "          tweetsdf = pd.DataFrame(tweets,columns=['Datetime', 'Tweet Id', 'Text','Coordinates','Location','GeoTagged'])\n",
    "\n",
    "          # Converting dataframe to CSV\n",
    "          tweetsdf.to_csv('{}-tweets.csv'.format(text_query)) \n",
    "\n",
    "    except BaseException as e:\n",
    "        print('failed on_status,',str(e))\n",
    "        time.sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1hOeCFq6M83k"
   },
   "outputs": [],
   "source": [
    "# play groundc cell\n",
    "tweets = []\n",
    "\n",
    "def search_query(text_query, count):\n",
    "    for tweet in api.search(q=text_query, count=count):\n",
    "        tweets.append((tweet.created_at,tweet.id,tweet.text,tweet.coordinates,tweet.user.location, tweet.user.geo_enabled))\n",
    "        tweetsdf = pd.DataFrame(tweets,columns=['Datetime', 'Tweet Id', 'Text','Coordinates','Location','GeoTagged'])\n",
    "        tweetsdf.to_csv('{}-tweets.csv'.format(text_query)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Status' object has no attribute 'places'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-79-b76166f2cad5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mquery\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'obama'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtweet\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mapi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m300\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mtest_tweets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtweet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreated_at\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtweet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtweet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoordinates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtweet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlocation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtweet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplaces\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtweet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgeo_enabled\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mdf_test_tweets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_tweets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Datetime'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Text'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'Coordinates'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'Location'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'Places'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'GeoTagged'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Status' object has no attribute 'places'"
     ]
    }
   ],
   "source": [
    "test_tweets = []\n",
    "query = 'obama'\n",
    "for tweet in api.search(q=query,count=300):\n",
    "    test_tweets.append((tweet.created_at,tweet.text,tweet.coordinates, tweet.user.location, tweet.places, tweet.user.geo_enabled))\n",
    "    df_test_tweets = pd.DataFrame(test_tweets, columns=['Datetime', 'Text','Coordinates','Location','Places','GeoTagged'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                                  37\n",
       "United States                      5\n",
       "California, USA                    2\n",
       "Georgia, USA                       2\n",
       "USA                                2\n",
       "Virginia, USA                      2\n",
       "Canada                             1\n",
       "York, Maine                        1\n",
       "United Kingdom                     1\n",
       "Lonestar State                     1\n",
       "Allegheny County                   1\n",
       "College Station, TX                1\n",
       "Scottsdale, AZ                     1\n",
       "Kentucky, USA                      1\n",
       "Port Harcourt Rivers state Nig     1\n",
       "Richardson,Texas                   1\n",
       "US                                 1\n",
       "La Empat√≠a de las Babosas          1\n",
       "A^2                                1\n",
       "Midwest, USA                       1\n",
       "Exton, PA                          1\n",
       "Indiana, USA                       1\n",
       "Tulsa, OK                          1\n",
       "  Texas                            1\n",
       "Minnesota, USA                     1\n",
       "atlanta                            1\n",
       "Cape Town, South Africa            1\n",
       "South Daytona, FL                  1\n",
       "Port St Lucie, FL                  1\n",
       "Indiana                            1\n",
       "La Safor                           1\n",
       "Denver                             1\n",
       "Harpers Ferry, WV                  1\n",
       " Global                            1\n",
       "Tugela Ferry , South Africa        1\n",
       "Sarasota, FL                       1\n",
       "Dallas, TX                         1\n",
       "Belton, MO                         1\n",
       "Alberta, Canada                    1\n",
       "Lexington & Concord                1\n",
       "Tennessee, USA                     1\n",
       "Scotland, United Kingdom           1\n",
       "Virginia                           1\n",
       "Louisiana, USA                     1\n",
       "South Carolina, USA                1\n",
       "Sanctuary and Rollers Warehous     1\n",
       "LIVE IN AMERICA AND PROUD .        1\n",
       "North Fort Myers, FL               1\n",
       "Lives in Virginia works in DC.     1\n",
       "Oregon City, OR                    1\n",
       "Manhattan, NY                      1\n",
       "BC Canada üçÅ                        1\n",
       "Texas                              1\n",
       "Connecticut, USA                   1\n",
       "California                         1\n",
       "Guwahati, India                    1\n",
       "Name: Location, dtype: int64"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test_tweets[\"Location\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wJcKa7Wrk_e2"
   },
   "source": [
    "## Query Function Calls\n",
    "Putting it all together and using functions created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dgK3Am48leWF"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "failed on_status, 'NoneType' object is not subscriptable\n"
     ]
    }
   ],
   "source": [
    "# Input search query to scrape tweets and name csv file\n",
    "# Max recent tweets pulls x amount of most recent tweets from that user\n",
    "text_query = 'poweroutage'\n",
    "count = 10_000\n",
    "# Calling function to query X amount of relevant tweets and create a CSV file\n",
    "df = text_query_to_csv(text_query, count)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Tweepy Twitter Scraper",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
